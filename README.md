# flash-attention 3 prebuilt wheels for H100

This repository hosts prebuilt Python wheels for [flash-attn v3](https://github.com/Dao-AILab/flash-attention), 
compiled for NVIDIA H100 GPUs.

## Available wheels
- **flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl**  
  - Python **3.9+** (abi3)  
  - Linux x86_64  
  - Built with CUDA support for **H100 (sm_90)**  

## Installation
Download the wheel from the [Releases](../../releases) page, then install with:

```bash
pip install flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl
```
